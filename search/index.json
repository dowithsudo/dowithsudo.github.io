[{"content":"Welcome To My Blog. Thanks for stopping by. I mean that.\nThis site is my personal blog. It stays simple on purpose. No marketing talk. No polished promises.\nI started this blog to write down my work in IT. Over time, it turned into something more personal. It became a place to record real situations, both technical and human. Servers fail. Networks act strange. Systems choose the worst timing. Life keeps going after the laptop shuts down.\nLogs and alerts pay the bills. Life still happens around them.\nHere is what I write about:\nReal IT troubleshooting from production systems Server and infrastructure notes from daily work Network problems that appear outside diagrams Technical reminders worth keeping Personal stories from normal days Lessons from work and life Some posts focus on technical detail. Some focus on experience. Many sit between both.\nIf it feels practical, it came from real work. If it feels personal, it came from a real moment.\nabout me My name is Danu.\nI live in Bekasi, Indonesia. I have worked in IT for more than 14 years. My days revolve around systems, networks, infrastructure, and things that tend to break after office hours.\nBy the way, this blog exists for a few reasons:\nWriting helps me think clearly Documentation should live outside ticket tools Real experience rarely appears in tutorials Not every lesson comes from technology Small moments often teach more than major incidents.\nBlack coffee stays close. It counts as part of the workflow.\nwhy this blog exists The goal stays simple.\nEach article aims to offer:\nUseful context Practical ideas References you can apply Not every setup matches your environment. That is normal. Networks differ. Constraints differ. Risks differ. Take what fits and adjust the rest.\nStability matters more than shiny setups that fail under load. Security stays a process, not a checklist.\nget in touch If you want to discuss a topic, talk about consulting, work on a project, or share a job opportunity, feel free to reach out. I am open to work at the moment.\nEmail: kamandanu.wijaya@gmail.com Phone or WhatsApp: +62 812 2233 129 Replies may take time. But i`ll try my best to reply soon as posible.\nclosing Thanks for visiting.\nI hope something here saves you time or prevents a late-night issue. If that happens, the documentation worked.\nEnjoy reading. Welcome to the blog.\n","date":"2025-12-13T00:00:00Z","image":"https://dowithsudo.com/p/welcome-to-my-blog/welcome_hu_101c5b80544c7662.webp","permalink":"https://dowithsudo.com/p/welcome-to-my-blog/","title":"welcome to my blog"},{"content":"Learning Terraform Basics I write this from daily ops experience. Terraform is a tool for people who hate guessing. It turns infrastructure into files you can read, review, and track. Logs matter, and state files matter even more.\nTerraform manages infrastructure using code. The code is plain text. You can store it in Git. You can review changes before touching production. That alone already saves sleep.\nThink of Terraform like a wiring diagram. You do not power anything yet. You only describe how cables should connect. Terraform reads the diagram. Then it builds exactly that.\nWhat Problem Terraform Solves Manual setup works. It works until it does not.\nCommon issues appear fast.\nServers differ between environments Changes are forgotten Rollback depends on memory Documentation drifts from reality Terraform fixes this by force. It only builds what the code says. No more hidden clicks. No more silent changes at midnight.\nCore Concepts You Must Understand Terraform looks simple at first. The basics decide everything later.\nProvider A provider is a bridge. It connects Terraform to a platform. AWS, Azure, GCP, and many others use providers.\nIf you manage AWS EC2. You use the AWS provider.\nResource A resource is one object. A server. A network. A firewall rule.\nEach resource has a type and a name. Terraform tracks each resource in its state.\nState State is the source of truth. Not the cloud console. Not your memory.\nThe state file records real infrastructure. Lose it and you lose control. Protect it like production data.\nPlan Plan shows what will change. Nothing executes yet.\nThis step prevents surprises. Always read the plan.\nApply Apply makes changes real. Terraform compares code and state. Then it acts.\nInstalling Terraform Use the official binary. Avoid random packages.\nExample for Linux.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Download Terraform binary # This gets the exact version we want wget https://releases.hashicorp.com/terraform/1.6.6/terraform_1.6.6_linux_amd64.zip # Unzip the binary # This extracts the terraform executable unzip terraform_1.6.6_linux_amd64.zip # Move binary to PATH # This allows terraform command anywhere sudo mv terraform /usr/local/bin/ # Verify installation # This confirms terraform is ready terraform version If this fails. Fix it now. Do not continue.\nFirst Terraform Project Structure Keep structure boring. Boring is stable.\nExample layout.\nmain.tf variables.tf outputs.tf terraform.tfstate Do not rename files randomly. Terraform loads all .tf files together.\nWriting Your First Terraform Code We start small. One local file. No cloud yet.\nThis removes noise.\nmain.tf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Configure Terraform settings # Required version avoids unexpected behavior terraform { required_version = \u0026#34;\u0026gt;= 1.6.0\u0026#34; } # Local file resource # This creates a file on the local system resource \u0026#34;local_file\u0026#34; \u0026#34;example\u0026#34; { # File content # This text will be written into the file content = \u0026#34;Terraform was here\u0026#34; # File name # Path where the file will be created filename = \u0026#34;./hello.txt\u0026#34; } This code does one thing. It creates a text file.\nThat is enough to learn the workflow.\nTerraform Workflow Step by Step This flow never changes. Learn it once.\nStep 1. Initialize 1 2 3 # Initialize Terraform project # This downloads required providers terraform init Init prepares the working directory. Run it once per project. Run it again if providers change.\nStep 2. Validate 1 2 3 # Validate configuration files # This checks syntax and basic errors terraform validate This catches mistakes early. Use it before plan.\nStep 3. Plan 1 2 3 # Show execution plan # This previews changes without applying terraform plan Read the output. Every line matters. If something looks wrong. Stop here.\nStep 4. Apply 1 2 3 # Apply changes # This creates or updates resources terraform apply Terraform asks for confirmation. Say yes only when ready.\nAfter apply. Check the file system. hello.txt should exist.\nUnderstanding State Behavior Terraform stores state locally by default. This is fine for learning. It is risky for teams.\nState maps code to real objects. Delete state and Terraform forgets everything.\nFor real projects. Use remote state. S3, GCS, or similar backends.\nVariables for Reusable Code Hardcoded values do not scale. Variables fix that.\nvariables.tf 1 2 3 4 5 6 7 8 9 # Define a variable # This allows flexible input variable \u0026#34;file_content\u0026#34; { # Variable type type = string # Default value default = \u0026#34;Terraform was here\u0026#34; } Update main.tf.\n1 2 3 4 5 6 7 8 # Use variable inside resource resource \u0026#34;local_file\u0026#34; \u0026#34;example\u0026#34; { # Content comes from variable content = var.file_content # File path stays the same filename = \u0026#34;./hello.txt\u0026#34; } Now the code adapts. No rewrite needed.\nOutputs for Visibility Outputs show useful data. They help other tools.\noutputs.tf 1 2 3 4 5 # Output file path # This displays after apply output \u0026#34;file_path\u0026#34; { value = local_file.example.filename } Run apply again. Terraform prints the output.\nSimple Analogy That Actually Works Terraform is not magic. It is a strict technician.\nYou give a checklist. It follows the checklist exactly.\nIf the checklist is wrong. Terraform is not polite. It still executes it.\nThat is why review matters.\nCommon Beginner Mistakes These appear every week.\nSkipping plan Editing resources manually Ignoring state files Mixing environments in one project Terraform rewards discipline. It punishes shortcuts.\nWhen Terraform Is a Bad Idea Terraform is not for everything.\nAvoid it for.\nOne time experiments Manual emergency fixes Systems without stable APIs Use the right tool. Sleep matters.\nFinal Notes Terraform basics are simple. The impact is not.\nTreat code like production. Review changes. Protect state. Write boring configs.\nIf it feels boring. You are doing it right.\nExample Using AWS This example uses AWS EC2. It stays minimal on purpose.\nYou need an AWS account. You need an IAM user. Use access key and secret key.\nProvider Configuration 1 2 3 4 5 6 7 # AWS provider configuration # This tells Terraform how to talk to AWS provider \u0026#34;aws\u0026#34; { # AWS region # Choose one and stay consistent region = \u0026#34;ap-southeast-1\u0026#34; } EC2 Instance Resource 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # EC2 instance resource # This creates a single virtual server resource \u0026#34;aws_instance\u0026#34; \u0026#34;lab_server\u0026#34; { # Amazon Linux AMI # AMI ID is region specific ami = \u0026#34;ami-0df7a207adb9748c7\u0026#34; # Instance size # Small enough for lab usage instance_type = \u0026#34;t3.micro\u0026#34; # Tags help tracking and cleanup tags = { Name = \u0026#34;terraform-lab-server\u0026#34; } } Run init, plan, and apply. Check AWS console after apply. The instance should exist.\nSeparating Lab and Production Never mix environments. It ends badly.\nUse separate directories.\nenvs/lab envs/production Each environment has its own state. Each environment has its own variables.\nExample structure.\nmodules/ec2 envs/lab/main.tf envs/production/main.tf Lab Environment Example 1 2 3 4 5 6 7 8 # Lab environment EC2 # Uses smaller instance and cheaper setup module \u0026#34;ec2\u0026#34; { source = \u0026#34;../../modules/ec2\u0026#34; instance_type = \u0026#34;t3.micro\u0026#34; name = \u0026#34;lab-server\u0026#34; } Production Environment Example 1 2 3 4 5 6 7 8 # Production environment EC2 # Uses more stable instance size module \u0026#34;ec2\u0026#34; { source = \u0026#34;../../modules/ec2\u0026#34; instance_type = \u0026#34;t3.medium\u0026#34; name = \u0026#34;production-server\u0026#34; } Same code. Different behavior. No risk of cross damage.\nRemote State and Locking Best Practice Local state does not scale. Teams overwrite each other.\nUse remote backend. Use locking.\nAWS S3 with DynamoDB works well.\nRemote State Configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Remote backend configuration # Stores state in S3 and locks via DynamoDB terraform { backend \u0026#34;s3\u0026#34; { # S3 bucket for state file bucket = \u0026#34;my-terraform-state-bucket\u0026#34; # Path inside the bucket key = \u0026#34;lab/terraform.tfstate\u0026#34; # AWS region region = \u0026#34;ap-southeast-1\u0026#34; # DynamoDB table for locking dynamodb_table = \u0026#34;terraform-locks\u0026#34; # Encrypt state at rest encrypt = true } } Why This Matters State contains sensitive data. IPs, IDs, and metadata.\nLocking prevents parallel apply. Parallel apply breaks infrastructure.\nCreate the S3 bucket first. Create DynamoDB table first. Terraform will not do that for you.\nOperational Notes From the Field Use one state per environment. Never share state files.\nProtect backend credentials. Rotate keys regularly.\nReview plan output as a team. One missed line can cost hours.\nTerraform works best with discipline. It does not forgive carelessness.\n","date":"2025-12-11T00:00:00Z","permalink":"https://dowithsudo.com/p/learn-terraform-basic/","title":"Learn Terraform Basic"},{"content":"Linux Server Hardening A practical, boring, and reliable approach (which is exactly what we want)\nI’ve hardened Linux servers for years. Web server, database server, jump host, you name it.\nOne lesson that never changes:\nMost incidents are not zero-days. They’re misconfigurations.\nSo this article is not about “next-gen AI security magic”. This is about making your Linux server less attractive, less noisy, and less forgiving when someone does something stupid.\nHardening is not a checklist. It’s a habit.\nWhat We’re Actually Protecting Against Let’s be realistic.\nMost Linux servers get compromised because of:\nExposed services no one remembers enabling Weak SSH setup No patching discipline Running everything as root “Temporary” firewall rules that lived forever Not because attackers are geniuses. But because admins are tired, rushed, or undocumented.\nSo we harden to:\nReduce attack surface Limit blast radius Make logs useful when things go sideways Baseline Assumptions Before we start, some ground rules:\nOS: Ubuntu Server / Debian-based (adjust if needed) You have root or sudo This is a server, not a personal laptop You’re okay trading convenience for stability Basic System Hardening I’ll go layer by layer. Do not skip steps just because “it works now”.\nUpdate the System If you skip this, stop here.\n1 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y Why this matters:\nMost exploits target known vulnerabilities Early patching avoids dependency chaos later Real-world note: I’ve seen servers breached just because openssh was three years old.\nCreate a Non-Root User Root is powerful. Root is also dangerous.\n1 2 3 4 5 # Create a new user sudo adduser danuadmin # Allow the user to run sudo sudo usermod -aG sudo danuadmin What this does:\nCreates a regular user with limited privileges Makes activity tracking clearer in logs Harden SSH Configuration Most attacks start here.\nEdit SSH configuration:\n1 sudo nano /etc/ssh/sshd_config Recommended settings:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Disable direct root login PermitRootLogin no # Disable password-based login PasswordAuthentication no # Restrict SSH access to specific users AllowUsers danuadmin # Enforce modern protocol Protocol 2 # Reduce brute-force attempts MaxAuthTries 3 Restart SSH service:\n1 sudo systemctl restart ssh Important: Test SSH key login before closing your session.\nUse SSH Key Authentication On your local machine:\n1 2 # Generate a strong SSH key ssh-keygen -t ed25519 -C \u0026#34;danu@laptop\u0026#34; Copy the key to the server:\n1 ssh-copy-id danuadmin@your-server-ip Why:\nKeys can’t be brute-forced Credentials don’t travel over the network Enable Firewall (UFW) Simple. Readable. Reliable.\n1 2 3 4 5 6 7 8 # Allow SSH access sudo ufw allow OpenSSH # Enable firewall sudo ufw enable # Verify rules sudo ufw status verbose For web servers:\n1 2 sudo ufw allow 80 sudo ufw allow 443 Disable Unused Services List listening services:\n1 sudo ss -tulnp Disable what you don’t need:\n1 2 sudo systemctl stop avahi-daemon sudo systemctl disable avahi-daemon Rule of thumb:\nIf you don’t know what it does, it shouldn’t be running.\nInstall Fail2Ban Cheap protection. Big noise reduction.\n1 sudo apt install fail2ban -y Create local config:\n1 2 sudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local sudo nano /etc/fail2ban/jail.local Example SSH jail:\n1 2 3 4 5 [sshd] enabled = true port = ssh maxretry = 3 bantime = 1h Restart Fail2Ban:\n1 sudo systemctl restart fail2ban File Permission Hygiene Find world-writable files:\n1 sudo find / -xdev -type f -perm -0002 -print Fix permissions:\n1 sudo chmod o-w /path/to/file Why:\nPrevents privilege escalation Protects sensitive configuration files Kernel Hardening (sysctl) Create hardening config:\n1 sudo nano /etc/sysctl.d/99-hardening.conf Add the following:\n1 2 3 4 5 6 7 8 9 # Disable IP source routing net.ipv4.conf.all.accept_source_route = 0 # Ignore ICMP redirects net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.default.accept_redirects = 0 # Enable SYN cookies net.ipv4.tcp_syncookies = 1 Apply settings:\n1 sudo sysctl --system Logging and Log Rotation Ensure logs don’t eat your disk:\n1 sudo apt install logrotate -y Check configuration:\n1 cat /etc/logrotate.conf No logs means no evidence. No rotation means future outage.\nAutomatic Security Updates (Optional) Install unattended upgrades:\n1 2 sudo apt install unattended-upgrades -y sudo dpkg-reconfigure unattended-upgrades Use with care:\nTest on staging first Not all production systems like auto updates Security is always contextual.\nFinal Thoughts Hardening is not paranoia. It’s predictability.\nA hardened server:\nBehaves consistently Fails loudly, not silently Produces logs you can trust Better to be tired during setup than exhausted during an incident.\nIf you want to go deeper:\nCIS Benchmark alignment Docker or Kubernetes hardening Database-specific hardening Say the word. Coffee is ready.\n","date":"2025-12-13T00:00:00Z","image":"https://dowithsudo.com/p/linux-server-hardening/cover_hu_5116f0c4af0f04a5.webp","permalink":"https://dowithsudo.com/p/linux-server-hardening/","title":"Linux Server Hardening"},{"content":"Backup MySQL Database to Google Drive Using rclone I have broken more backups than I want to admit. Not because the tools were bad, but because the process was vague, undocumented, or too clever for its own good.\nThis article is about a boring but reliable way to back up a MySQL database and push it to Google Drive using rclone. Nothing fancy. Just something you can trust at 02:00 AM when the alert tone is already louder than your coffee machine.\nI assume you are running Linux, you have shell access, and you prefer logs over hope.\nWhy rclone + Google Drive Short answer: it works.\nLonger answer:\nGoogle Drive is cheap, redundant, and usually already approved by management rclone is stable, scriptable, and well documented No vendor lock-in feeling Easy to rotate, easy to audit Think of it like this:\nmysqldump is the guy packing your stuff gzip is the vacuum bag rclone is the delivery truck Each does one job. That is already a good sign.\nWhat We Are Building At the end, you will have:\nA MySQL dump Compressed to save space Uploaded to Google Drive Logged properly Ready to be scheduled via cron No magic. No GUI. No guessing.\nPrerequisites Before touching anything, make sure these exist:\nLinux server (Ubuntu, Debian, Alma, Rocky, etc) MySQL or MariaDB A Google account Root or sudo access Coffee. Black. Optional but recommended Check binaries first:\n1 which mysqldump gzip rclone If something is missing, install it. Do not continue until this is clean.\nStep 1: Install rclone On most modern Linux systems:\n1 curl https://rclone.org/install.sh | sudo bash Verify installation:\n1 rclone version If this fails, stop here. Debug first. Backup scripts built on broken tools are just future incidents.\nStep 2: Configure rclone for Google Drive Run interactive config:\n1 rclone config Basic flow:\nChoose n for new remote Name it something sane, example: gdrive-backup Storage type: drive Client ID and secret: leave empty unless you know why you should not Scope: full access (default) Auto config: yes Your browser will open. Login. Approve.\nTest it:\n1 rclone lsd gdrive-backup: If you see folders, good. If not, fix this before moving on.\nStep 3: Prepare Directory Structure I like predictable paths. Future me appreciates this.\n1 2 mkdir -p /opt/backup/mysql mkdir -p /var/log/backup Why:\n/opt/backup/mysql for dump files /var/log/backup for logs No guessing later.\nStep 4: Create the Backup Script Create the script:\n1 nano /opt/backup/mysql/mysql_backup_to_gdrive.sh Full script below. Read it. Do not just copy blindly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #!/bin/bash # Exit immediately if a command exits with a non-zero status # This prevents silent failures set -e # Variables DATE=$(date +\u0026#34;%Y-%m-%d_%H-%M-%S\u0026#34;) BACKUP_DIR=\u0026#34;/opt/backup/mysql\u0026#34; LOG_FILE=\u0026#34;/var/log/backup/mysql_backup.log\u0026#34; DB_NAME=\u0026#34;your_database_name\u0026#34; DB_USER=\u0026#34;backup_user\u0026#34; DB_PASS=\u0026#34;strong_password\u0026#34; REMOTE_NAME=\u0026#34;gdrive-backup\u0026#34; REMOTE_DIR=\u0026#34;mysql-backup\u0026#34; # Log start time echo \u0026#34;[$(date)] Backup started\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; # Create MySQL dump # --single-transaction avoids table locking for InnoDB # --routines and --triggers are often forgotten, so we include them mysqldump \\ --user=\u0026#34;$DB_USER\u0026#34; \\ --password=\u0026#34;$DB_PASS\u0026#34; \\ --single-transaction \\ --routines \\ --triggers \\ \u0026#34;$DB_NAME\u0026#34; \\ | gzip \u0026gt; \u0026#34;$BACKUP_DIR/${DB_NAME}_${DATE}.sql.gz\u0026#34; # Upload to Google Drive using rclone # copy is safer than sync for backups rclone copy \\ \u0026#34;$BACKUP_DIR\u0026#34; \\ \u0026#34;$REMOTE_NAME:$REMOTE_DIR\u0026#34; \\ --log-file=\u0026#34;$LOG_FILE\u0026#34; \\ --log-level INFO # Optional: remove local backups older than 7 days # Adjust this if your disk is small find \u0026#34;$BACKUP_DIR\u0026#34; -type f -mtime +7 -name \u0026#34;*.sql.gz\u0026#34; -delete # Log end time echo \u0026#34;[$(date)] Backup finished\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; Script Explanation (Important) Key parts that matter in real life:\nset -e\nScript stops on error instead of pretending everything is fine --single-transaction\nKeeps dump consistent without locking tables gzip\nSaves space and bandwidth rclone copy instead of sync\nBackup should only add, not delete remotely Log file\nWhen something breaks, logs talk. Scripts do not. Step 5: Secure the Script Make it executable:\n1 chmod 700 /opt/backup/mysql/mysql_backup_to_gdrive.sh Why:\nDatabase credentials are inside Least privilege is not optional If possible, consider:\nUsing a .my.cnf file Or a dedicated MySQL backup user with limited privileges Security is a process, not a checkbox.\nStep 6: Test Manually Run it once:\n1 /opt/backup/mysql/mysql_backup_to_gdrive.sh Check:\nLocal backup file exists File exists in Google Drive Log file has no errors If something fails, fix it now. Do not postpone debugging.\nStep 7: Schedule with Cron Edit crontab:\n1 crontab -e Example, daily at 01:30 AM:\n1 30 1 * * * /opt/backup/mysql/mysql_backup_to_gdrive.sh After that:\nMonitor logs for a few days Verify file sizes Try restoring once (yes, really) A backup never tested is just a theory.\nCommon Pitfalls I See Too Often Backup runs, but never tested restore Using sync and accidentally deleting old backups No log rotation Credentials with full root access Assuming Google Drive means infinite retention None of these look dangerous. Until they are.\nClosing Notes This setup is not glamorous.\nBut it is:\nPredictable Auditable Easy to explain to the next admin If you want encryption, retention policies, or multi-region strategy, build on top of this. Do not replace it with something you do not fully understand.\nWhen choosing between looking smart and being useful, I choose useful. Every time.\nNow go document this. Your future self will thank you.\n","date":"2025-12-14T00:00:00Z","image":"https://dowithsudo.com/p/backup-database-to-google-drive-with-rclone/cover_hu_f6b2592993b6aadd.webp","permalink":"https://dowithsudo.com/p/backup-database-to-google-drive-with-rclone/","title":"Backup Database To Google Drive With Rclone"},{"content":"Why Docker Exists (From Real Life) A few years back, I got the classic line:\n\u0026ldquo;It works on my laptop.\u0026rdquo;\nDifferent OS, different library versions, different configs. Same app, different behavior. Logs didn’t lie, but they didn’t help much either.\nDocker showed up as a packaging standard.\nSame app Same dependencies Same behavior From dev laptop to production server.\nNot magic. Just consistency.\nWhat Docker Actually Is Let’s keep it grounded. Docker is a tool that lets you package an application together with everything it needs to run (code, libraries, config) into a single unit called a container.\nContainer vs VM Docker containers are not virtual machines.\nVM\nFull OS Heavy Slower to boot Container\nShares host kernel Lightweight Starts in seconds Think of containers as isolated processes with rules.\nCore Concepts You Must Understand No buzzwords. Just essentials.\nImage Blueprint Read-only Built once, run many times Example:\nnginx:latest mysql:8.0 Container Running instance of an image Has lifecycle Create → Start → Stop → Remove\nDockerfile Recipe Defines how an image is built If documentation is missing, Dockerfile becomes your last line of defense.\nVolume Persistent data Survives container removal Databases without volumes are just ticking time bombs.\nInstalling Docker (Linux Example) I mostly work on Linux servers. Ubuntu in this case.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Update package index # Keeps local package list in sync sudo apt update # Install required packages # Allows apt to use repositories over HTTPS sudo apt install -y ca-certificates curl gnupg # Add Docker official GPG key # Used to verify Docker packages authenticity sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg # Add Docker repository # Tells apt where to download Docker packages echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Install Docker Engine sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io Quick sanity check:\n1 2 3 # Check Docker version # Confirms Docker is installed and reachable docker --version If this fails, stop here. Check logs. Don’t continue blindly.\nYour First Container Let’s start boring. Nginx.\n1 2 3 4 5 # Run nginx container # -d : run in background # -p : map host port to container port # --name : easier to reference later docker run -d -p 8080:80 --name my-nginx nginx:latest What just happened:\nDocker pulled the image Created a container Exposed port 80 inside container to 8080 on host Test it:\n1 2 # Simple HTTP request to verify container is responding curl http://localhost:8080 If this works, Docker networking is fine. For now.\nUnderstanding Dockerfile (Hands-on) A Dockerfile is a simple text file that tells Docker how to build an image step by step, when Docker reads the Dockerfile, it follows the instructions from top to bottom and creates the same environment every time. A Dockerfile explains what to install, how to configure it, and how to run the app — automatically and consistently.\nNow we build something ourselves.\nSimple Dockerfile Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Use official lightweight Python image # Acts as base OS and runtime FROM python:3.11-slim # Set working directory inside container # All commands will run from here WORKDIR /app # Copy local files to container # Keeps application code inside image COPY . /app # Install dependencies # --no-cache-dir reduces image size RUN pip install --no-cache-dir flask # Expose application port # Purely documentation, not firewall EXPOSE 5000 # Default command when container starts # Runs Flask app CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Every line matters. If you don’t understand one, stop and read.\nBuilding and Running Custom Image 1 2 3 # Build Docker image # -t assigns a readable name docker build -t my-flask-app . Run it:\n1 2 3 # Run container from custom image # Maps Flask port to host docker run -d -p 5000:5000 --name flask-test my-flask-app Check status:\n1 2 3 # List running containers # Confirms container health docker ps If it exits immediately, don’t guess. Check logs.\n1 2 3 # View container logs # First place to look when something breaks docker logs flask-test Logs are honest. People aren’t always.\nData Persistence with Volumes Containers die. Data should not.\n1 2 3 4 5 6 7 # Run MySQL with volume # -v maps host directory to container data path docker run -d \\ --name mysql-db \\ -e MYSQL_ROOT_PASSWORD=secret \\ -v mysql_data:/var/lib/mysql \\ mysql:8.0 Why volume matters:\nContainer removed → data still exists Upgrade container without losing DB Seen too many incidents caused by skipping this.\nBasic Docker Commands You’ll Use Daily 1 2 3 4 5 6 7 8 9 10 11 # Stop container gracefully docker stop \u0026lt;container_name\u0026gt; # Remove container docker rm \u0026lt;container_name\u0026gt; # Remove image docker rmi \u0026lt;image_name\u0026gt; # Inspect container details docker inspect \u0026lt;container_name\u0026gt; I use inspect more than I’d like to admit.\nThings Beginners Usually Break Based on real incidents.\nNo volume for database Exposing services without firewall rules Running everything as root No image version pinning Docker doesn’t remove responsibility. It just moves it.\nFinal Notes from the Coffee Mug Docker is not about being fancy.\nIt’s about:\nRepeatability Predictability Easier recovery at 2 AM If your setup is boring and well-documented, you’re doing it right.\nNext step after this:\nDocker Compose Basic container security Resource limits But that’s another cup of coffee.\n","date":"2025-12-13T00:00:00Z","image":"https://dowithsudo.com/p/basic-docker/cover_hu_a20a450cd5cffd1c.webp","permalink":"https://dowithsudo.com/p/basic-docker/","title":"Basic Docker"},{"content":"Learning Linux File Permissions (Without Losing Your Sanity) I’ve broken enough systems to learn one thing the hard way: file permissions in Linux are not optional knowledge.\nSooner or later, something will not start, a service will refuse to read a config file, or a script will work perfectly… until you move it to production.\nWhen that happens, I don’t guess. I check logs, then permissions. Usually in that order, while sipping black coffee.\nThis article is a practical, real-world guide to Linux file permissions. Not academic. Not fancy. Just the stuff that actually saves your night.\nWhy File Permissions Matter (In Real Life) Think of Linux permissions like office access cards.\nSome people can enter the building Some can only enter certain rooms Some should never be there at all If permissions are wrong:\nWeb server can’t read files Backup scripts fail silently Config files get exposed Or worse, writable by everyone Security incidents often start with something simple. Like chmod 777 done in a hurry.\nYes, I’ve seen it. More than once.\nThe Basics: Ownership and Permission Model Every file and directory in Linux has:\nOwner (user) Group Others (everyone else) And three basic permissions:\nr = read w = write x = execute Let’s look at a real example.\n1 ls -l example.sh Example output:\n1 -rwxr-x--- 1 danu devops 1024 Mar 10 10:00 example.sh Breakdown:\n- : regular file rwx : owner permissions r-x : group permissions --- : others permissions Meaning:\nOwner can read, write, execute Group can read and execute Others get nothing This is boring. And very important.\nNumeric (Octal) Permissions Explained If symbolic permissions feel abstract, numeric ones are just math. Simple math.\nPermission Value read (r) 4 write (w) 2 execute(x) 1 Add them up:\n7 = 4+2+1 = rwx 6 = 4+2 = rw- 5 = 4+1 = r-x 4 = 4 = r\u0026ndash; Example:\n1 chmod 750 example.sh Meaning:\nOwner: 7 (rwx) Group: 5 (r-x) Others: 0 (\u0026mdash;) This is one of my default choices for scripts. Enough access to run. Not enough to cause drama.\nStep-by-Step: Checking Permissions Step 1: Identify the Problem Something fails. Don’t panic. Check logs first.\n1 journalctl -xe Or application logs. Permissions issues usually scream pretty clearly.\nStep 2: Inspect File Permissions 1 ls -l /path/to/file For directories:\n1 ls -ld /path/to/directory Remember:\nFiles need read to be read Directories need execute to be accessed This trips people up. A lot.\nStep-by-Step: Changing Permissions Safely Using chmod (Symbolic) 1 2 chmod u+x script.sh # u+x : add execute permission for the owner only 1 2 chmod g-w config.conf # g-w : remove write permission from group This is safer when you don’t want surprises.\nUsing chmod (Numeric) 1 2 3 4 chmod 640 config.conf # 6 (rw-) owner can read and write # 4 (r--) group can read # 0 (---) others get nothing For sensitive configs, this is my go-to. Especially on production servers.\nOwnership: chown and chgrp Permissions alone are useless if ownership is wrong.\nChange Owner and Group 1 2 3 chown danu:devops example.sh # danu : new owner # devops : new group Recursive (be careful):\n1 2 3 chown -R www-data:www-data /var/www/app # -R applies changes recursively # Always double-check the path Recursive commands are powerful. And unforgiving. Like production.\nDirectories: The Special Case Directories behave differently.\nr : list files w : create/delete files x : enter directory Example:\n1 2 3 4 chmod 750 /data/backups # Owner can manage backups # Group can access # Others locked out No execute permission? You can see the directory. But you can’t enter it.\nLinux is very literal.\nCommon Mistakes I Still See Using 777 as a quick fix Forgetting execute permission on directories Running services as root unnecessarily No documentation about why permissions were changed Quick fixes age badly. Especially at 2 AM.\nA Simple Permission Checklist (Realistic One) Before moving to production:\nWho owns the file? Which service actually needs access? Read-only or writable? Directory permissions checked? Documented? Security is a process. Not a checkbox.\nFinal Thoughts Linux file permissions look simple. Until they aren’t.\nLearn them early. Use them deliberately. Document your decisions.\nIt’s cheaper than an incident. And easier than explaining to management why a config file was world-writable.\nNow excuse me. My coffee is getting cold.\n","date":"2025-12-13T00:00:00Z","image":"https://dowithsudo.com/p/linux-file-permision/cover_hu_a34052f12713fa84.webp","permalink":"https://dowithsudo.com/p/linux-file-permision/","title":"Linux File Permision"}]